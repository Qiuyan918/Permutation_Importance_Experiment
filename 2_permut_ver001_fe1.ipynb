{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4806c1f-64b7-40fd-bee4-9e8ddd76056a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !mkdir /root/.kaggle\n",
    "# !cp /storage/kaggle.json /root/.kaggle/kaggle.json\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d raddar/amex-data-integer-dtypes-parquet-format -p /storage/data --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c914b0-ca21-446c-8804-d7a489feb685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 12 03:54:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000     Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 33%   26C    P8     3W / 230W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb24357-c732-4028-babc-a57dce49156d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.parquet',\n",
       " '__notebook_source__.ipynb',\n",
       " 'cust_type.parquet',\n",
       " 'test.parquet',\n",
       " 'train_labels.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/storage/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b67f94e-942d-4bd0-a1a2-2c9929c02f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np \n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import cupy, cudf\n",
    "from numba import cuda\n",
    "from cuml import ForestInference\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "print('XGB Version',xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8e9cad-9d94-4372-b3c8-66306c8a4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = '/storage/data' # 将CWD换成你的数据目录\n",
    "SEED = 42\n",
    "NAN_VALUE = -127 # will fit in int8\n",
    "FOLDS = 10\n",
    "RAW_CAT_FEATURES = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d4e5a-4c3a-49bd-8d75-200f8cd863ed",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9225222-07e5-403a-85e8-f6b69f03b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_auc(y_true, y_prob):\n",
    "    y_true = cupy.asarray(y_true)\n",
    "    y_true = y_true[cupy.argsort(y_prob)]\n",
    "    cumfalses = cupy.cumsum(1-y_true)\n",
    "    nfalse = cumfalses[-1]\n",
    "    auc = (y_true * cumfalses).sum()\n",
    "    auc = auc / (nfalse * (len(y_true) - nfalse))\n",
    "    return auc\n",
    "\n",
    "def eval_auc(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'auc', fast_auc(labels, preds), True\n",
    "\n",
    "# NEEDED WITH DeviceQuantileDMatrix BELOW\n",
    "class IterLoadForDMatrix(xgb.core.DataIter):\n",
    "    def __init__(self, df=None, features=None, target=None, batch_size=256*512):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.df = df\n",
    "        self.it = 0 # set iterator to 0\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self):\n",
    "        '''Reset the iterator'''\n",
    "        self.it = 0\n",
    "\n",
    "    def next(self, input_data):\n",
    "        '''Yield next batch of data.'''\n",
    "        if self.it == self.batches:\n",
    "            return 0 # Return 0 when there's no more batch.\n",
    "        \n",
    "        a = self.it * self.batch_size\n",
    "        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n",
    "        dt = cudf.DataFrame(self.df.iloc[a:b])\n",
    "        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n",
    "        self.it += 1\n",
    "        return 1\n",
    "\n",
    "def train_xgb_params(params, train, val_folds, features, early_stopping_rounds, num_boost_round, verbose_eval, data_seed, model_seed, ver='model', save_dir='./'):\n",
    "    importances = []\n",
    "    oof = []\n",
    "    skf = KFold(n_splits=FOLDS, shuffle=True, random_state=data_seed)\n",
    "\n",
    "    for fold in val_folds:\n",
    "        Xy_train = IterLoadForDMatrix(train.loc[train['fold'] != fold], features, 'target')\n",
    "        X_valid = train.loc[train['fold'] == fold, features]\n",
    "        y_valid = train.loc[train['fold'] == fold, 'target']\n",
    "\n",
    "        dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n",
    "        dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "\n",
    "        # TRAIN MODEL FOLD K\n",
    "        # XGB MODEL PARAMETERS\n",
    "        xgb_parms = { \n",
    "            'eval_metric':'auc',\n",
    "            'objective':'binary:logistic',\n",
    "            'tree_method':'gpu_hist',\n",
    "            'predictor':'gpu_predictor',\n",
    "            'random_state':model_seed,\n",
    "        }    \n",
    "        xgb_parms.update(params)\n",
    "        model = xgb.train(xgb_parms, \n",
    "                    dtrain=dtrain,\n",
    "                    evals=[(dtrain,'train'), (dvalid,'valid')],\n",
    "                    num_boost_round=num_boost_round, \n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose_eval=verbose_eval,\n",
    "                    ) \n",
    "        best_iter = model.best_iteration\n",
    "        if save_dir:\n",
    "            model.save_model(os.path.join(save_dir, f'{ver}_fold{fold}_{best_iter}.xgb'))\n",
    "\n",
    "        # GET FEATURE IMPORTANCE FOR FOLD K\n",
    "        dd = model.get_score(importance_type='weight')\n",
    "        df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n",
    "        df = df.set_index('feature')\n",
    "        importances.append(df)        \n",
    "        \n",
    "        # INFER OOF FOLD K\n",
    "        oof_preds = model.predict(dvalid, iteration_range=(0,model.best_iteration+1))\n",
    "        if verbose_eval:\n",
    "            acc = fast_auc(y_valid.values, oof_preds)\n",
    "            print(f'acc_{fold}', acc)\n",
    "\n",
    "        # SAVE OOF\n",
    "        df = train.loc[train['fold'] == fold, ['customer_ID','target'] ].copy()\n",
    "        df['oof_pred'] = oof_preds\n",
    "        oof.append( df )\n",
    "\n",
    "        del Xy_train, df, X_valid, y_valid, dtrain, dvalid, model\n",
    "        _ = gc.collect()\n",
    "\n",
    "    importances = pd.concat(importances, axis=1)\n",
    "    oof = cudf.concat(oof, axis=0, ignore_index=True).set_index('customer_ID')\n",
    "    acc = fast_auc(oof.target.values, oof.oof_pred.values)\n",
    "    return acc, oof, importances\n",
    "\n",
    "def clear_file(data_dir):\n",
    "    files = [c for c in os.listdir(data_dir) if '.ipynb_checkpoints' not in c]\n",
    "    for file in files:\n",
    "        os.remove(os.path.join(data_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8aaa7-4a86-4887-989e-49cb84342a03",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b286b691-0aad-40b0-92e3-8d732b641992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = cudf.read_csv(os.path.join(CWD, 'train_labels.csv'))\n",
    "train['customer_ID'] = train['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "\n",
    "train_cust = train[['customer_ID']]\n",
    "CUST_SPLITS = train_cust.sample(frac=0.2, random_state=SEED)\n",
    "CUST_SPLITS['fold'] = 9999\n",
    "\n",
    "train_cv_cust = train_cust[~train_cust['customer_ID'].isin(CUST_SPLITS['customer_ID'].values)].reset_index(drop=True)\n",
    "skf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "for fold,(train_idx, valid_idx) in enumerate(skf.split(train_cv_cust)):\n",
    "    df = train_cv_cust.loc[valid_idx, ['customer_ID']]\n",
    "    df['fold'] = fold\n",
    "    CUST_SPLITS = cudf.concat([CUST_SPLITS, df])\n",
    "CUST_SPLITS = CUST_SPLITS.set_index('customer_ID')\n",
    "CUST_SPLITS.to_csv('CUST_SPLITS.csv')\n",
    "\n",
    "del train, train_cust\n",
    "_ = gc.collect()\n",
    "\n",
    "CUST_SPLITS['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e8ad8-2bc2-4171-9f6d-fa81aac15340",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a986d3-2217-4647-9c66-217577f9eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data...\n",
      "shape of data: (5531451, 190)\n",
      "shape after engineering (458913, 918)\n",
      "There are 918 features!\n"
     ]
    }
   ],
   "source": [
    "def read_file(path = '', usecols = None):\n",
    "    # LOAD DATAFRAME\n",
    "    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n",
    "    else: df = cudf.read_parquet(path)\n",
    "    # REDUCE DTYPE FOR CUSTOMER AND DATE\n",
    "    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    df.S_2 = cudf.to_datetime( df.S_2 )\n",
    "    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n",
    "    #df = df.sort_values(['customer_ID','S_2'])\n",
    "    #df = df.reset_index(drop=True)\n",
    "    # FILL NAN\n",
    "    df = df.fillna(NAN_VALUE) \n",
    "    print('shape of data:', df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('Reading train data...')\n",
    "train = read_file(path=os.path.join(CWD, 'train.parquet'))\n",
    "\n",
    "def process_and_feature_engineer(df):\n",
    "    # FEATURE ENGINEERING FROM \n",
    "    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n",
    "    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n",
    "    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "    num_features = [col for col in all_cols if col not in cat_features]\n",
    "\n",
    "    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
    "\n",
    "    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n",
    "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
    "\n",
    "    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n",
    "    del test_num_agg, test_cat_agg\n",
    "    print('shape after engineering', df.shape )\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = process_and_feature_engineer(train)\n",
    "train = train.fillna(NAN_VALUE)\n",
    "\n",
    "# ADD TARGETS\n",
    "targets = cudf.read_csv(os.path.join(CWD, 'train_labels.csv'))\n",
    "targets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n",
    "targets = targets.set_index('customer_ID')\n",
    "train = train.merge(targets, left_index=True, right_index=True, how='left')\n",
    "train.target = train.target.astype('int8')\n",
    "del targets\n",
    "\n",
    "# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\n",
    "train = train.sort_index().reset_index()\n",
    "\n",
    "# FEATURES\n",
    "FEATURES = list(train.columns[1:-1])\n",
    "print(f'There are {len(FEATURES)} features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf5d876-3464-4af5-94d4-0a0c70ea8345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>P_2_std</th>\n",
       "      <th>P_2_min</th>\n",
       "      <th>P_2_max</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>D_39_mean</th>\n",
       "      <th>D_39_std</th>\n",
       "      <th>D_39_min</th>\n",
       "      <th>D_39_max</th>\n",
       "      <th>...</th>\n",
       "      <th>D_64_last</th>\n",
       "      <th>D_64_nunique</th>\n",
       "      <th>D_66_count</th>\n",
       "      <th>D_66_last</th>\n",
       "      <th>D_66_nunique</th>\n",
       "      <th>D_68_count</th>\n",
       "      <th>D_68_last</th>\n",
       "      <th>D_68_nunique</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9223193039457028513</td>\n",
       "      <td>0.974068</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.964483</td>\n",
       "      <td>1.002478</td>\n",
       "      <td>1.001372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9223189665817919541</td>\n",
       "      <td>0.802447</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0.694073</td>\n",
       "      <td>0.828761</td>\n",
       "      <td>0.694073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9223188534444851899</td>\n",
       "      <td>0.791203</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.786647</td>\n",
       "      <td>0.794826</td>\n",
       "      <td>0.787945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9223173911659837606</td>\n",
       "      <td>0.115666</td>\n",
       "      <td>0.078554</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.252421</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>6.144625</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9223126996485486147</td>\n",
       "      <td>0.978507</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.910546</td>\n",
       "      <td>1.009644</td>\n",
       "      <td>1.009644</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.277350</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 921 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           customer_ID  P_2_mean   P_2_std   P_2_min   P_2_max  P_2_last  \\\n",
       "0 -9223193039457028513  0.974068  0.013094  0.964483  1.002478  1.001372   \n",
       "1 -9223189665817919541  0.802447  0.038025  0.694073  0.828761  0.694073   \n",
       "2 -9223188534444851899  0.791203  0.002688  0.786647  0.794826  0.787945   \n",
       "3 -9223173911659837606  0.115666  0.078554  0.038207  0.252421  0.040486   \n",
       "4 -9223126996485486147  0.978507  0.029026  0.910546  1.009644  1.009644   \n",
       "\n",
       "   D_39_mean  D_39_std  D_39_min  D_39_max  ...  D_64_last  D_64_nunique  \\\n",
       "0   0.000000  0.000000         0         0  ...          0             1   \n",
       "1   0.000000  0.000000         0         0  ...          0             1   \n",
       "2   0.000000  0.000000         0         0  ...          3             2   \n",
       "3   4.384615  6.144625         0        17  ...          0             2   \n",
       "4   0.076923  0.277350         0         1  ...          0             1   \n",
       "\n",
       "   D_66_count  D_66_last  D_66_nunique  D_68_count  D_68_last  D_68_nunique  \\\n",
       "0          13         -1             1          13          6             1   \n",
       "1          13         -1             1          13          6             1   \n",
       "2          13         -1             1          13          5             1   \n",
       "3          13         -1             1          13          6             2   \n",
       "4          13         -1             1          13          6             1   \n",
       "\n",
       "   target  fold  \n",
       "0       0     1  \n",
       "1       0     4  \n",
       "2       0     4  \n",
       "3       1     0  \n",
       "4       0     3  \n",
       "\n",
       "[5 rows x 921 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADD CUST SPLITS\n",
    "CUST_SPLITS = cudf.read_csv('CUST_SPLITS.csv').set_index('customer_ID')\n",
    "train = train.set_index('customer_ID').merge(CUST_SPLITS, left_index=True, right_index=True, how='left')\n",
    "test = train[train['fold'] == 9999]\n",
    "train = train[train['fold'] != 9999]\n",
    "train = train.sort_index().reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b628a-0454-4dbe-aa6f-c13f24fe21ba",
   "metadata": {},
   "source": [
    "# Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60828e9f-d6b9-4ace-b08c-5b46dfcb4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permutation_importance(model_path, data, model_features, shuffle_features, shuffle_times=5):\n",
    "    permutation_importance = pd.DataFrame(columns=['feature', 'metric', 'shuffle_idx'])\n",
    "    model = ForestInference.load(model_path, model_type='xgboost', output_class=True)\n",
    "    preds = model.predict_proba(data[model_features])[1].values\n",
    "    acc = fast_auc(data['target'].values, preds)\n",
    "    permutation_importance.loc[permutation_importance.shape[0]] = ['original', cupy.asnumpy(acc), 0] \n",
    "    for col in tqdm(shuffle_features):\n",
    "        value = data[col].copy().values\n",
    "        for i in np.arange(shuffle_times):\n",
    "            np.random.seed(i)\n",
    "            data[col] = cupy.random.permutation(data[col].copy().values)\n",
    "            preds = model.predict_proba(data[model_features])[1].values\n",
    "            new_acc = fast_auc(data['target'].values, preds)\n",
    "            permutation_importance.loc[permutation_importance.shape[0]] = [col, cupy.asnumpy(new_acc), i] \n",
    "        data[col] = value\n",
    "    return permutation_importance\n",
    "\n",
    "def agg_permu_files(permu_dir, count_threshold, retrain_idx):\n",
    "    permu_files = [c for c in os.listdir(permu_dir) if ('.csv' in c) and (f'permutation_importance_{retrain_idx}' in c)]\n",
    "    permutation_importance_all = []\n",
    "    for file in permu_files:\n",
    "        df = pd.read_csv(os.path.join(permu_dir, file))\n",
    "        df['permut_idx'] = file.split('_')[-2]\n",
    "        permutation_importance_all.append(df)\n",
    "    permutation_importance_all = pd.concat(permutation_importance_all)\n",
    "    # original_acc\n",
    "    original_acc = permutation_importance_all[permutation_importance_all['feature'] == 'original'].set_index(['fold', 'permut_idx'])[['metric']]\n",
    "    original_acc.rename({'metric': 'metric_ori'}, axis=1, inplace=True)\n",
    "    permutation_importance_all = permutation_importance_all.set_index(['fold', 'permut_idx']).merge(original_acc, left_index=True, right_index=True, how='left')\n",
    "    permutation_importance_all['metric_diff_ratio'] = (permutation_importance_all['metric_ori'] - permutation_importance_all['metric']) / permutation_importance_all['metric_ori']\n",
    "    permutation_importance_all.reset_index(inplace=True)\n",
    "    # random_acc\n",
    "    random_acc = permutation_importance_all[permutation_importance_all['feature'] == 'random'].groupby(['permut_idx'])[['metric_diff_ratio']].agg(['mean', 'std'])\n",
    "    random_acc.columns = ['random_mean', 'random_std']\n",
    "    permutation_importance_all.reset_index(inplace=True)\n",
    "\n",
    "    permutation_importance_agg = permutation_importance_all[permutation_importance_all['feature'] != 'random'].groupby(['feature', 'permut_idx'])['metric_diff_ratio'].agg(['min', 'max','mean', 'std', 'count'])\n",
    "    permutation_importance_agg['z'] = permutation_importance_agg['mean'] / permutation_importance_agg['std']\n",
    "    if count_threshold:\n",
    "        permutation_importance_agg = permutation_importance_agg[permutation_importance_agg['count'] == count_threshold]\n",
    "    permutation_importance_agg = permutation_importance_agg.reset_index().set_index('permut_idx')\n",
    "    permutation_importance_agg['z'] = permutation_importance_agg['mean'] / permutation_importance_agg['std']\n",
    "    permutation_importance_agg = permutation_importance_agg.merge(random_acc, left_index=True, right_index=True, how='left')\n",
    "    permutation_importance_agg['random_z'] = permutation_importance_agg['random_mean'] / permutation_importance_agg['random_std']\n",
    "    permutation_importance_agg = permutation_importance_agg.reset_index().set_index('feature')\n",
    "    return permutation_importance_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f19c37-0950-48f4-8b00-dcfabc01f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_13m = {\n",
    "    'max_depth':4, \n",
    "    'learning_rate':0.1, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67e4e17b-e5a6-4e9c-9741-8a80cbd85ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD RANDOM FEATURE\n",
    "np.random.seed(SEED)\n",
    "train['random'] = np.random.normal(loc=1, scale=1, size=train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c5ea91-8fdf-42d1-ade1-2ff5e84b6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_107_max', 'D_41_mean', 'D_77_min', 'R_19_mean', 'D_131_min', 'B_41_max', 'D_76_max', 'D_91_max', 'D_54_mean', 'D_84_max']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "np.random.shuffle(FEATURES)\n",
    "FEATURES = FEATURES[:300]\n",
    "print(FEATURES[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fb880e-9c4f-495a-aa6e-c275e039f21c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES: 300; retrain_idx: 1; feature_split_num: 1; sub_features_num: 300\n",
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 0;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [02:36<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 1;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [02:40<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 2;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [02:41<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 3;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [02:44<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 4;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [02:45<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 5;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [02:36<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 6;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [02:38<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 7;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 8;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [02:39<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ sub_features_idx: 0; start_idx: 0, end_idx: 300, fold: 9;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [02:39<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "permu_dir = './permut_13m_ver001_retrain'\n",
    "if not os.path.isdir(permu_dir): os.mkdir(permu_dir)\n",
    "clear_file(permu_dir)\n",
    "\n",
    "retrain_idx = 1\n",
    "val_folds = [0,1,2,3,4,5,6,7,8,9]\n",
    "shuffle_times = 10\n",
    "\n",
    "while True:\n",
    "    # Features\n",
    "    sub_features_num = len(FEATURES)\n",
    "    feature_split_num = max(round(len(FEATURES) / sub_features_num), 1) \n",
    "    sub_features_num = len(FEATURES) // feature_split_num\n",
    "    print(f'FEATURES: {len(FEATURES)}; retrain_idx: {retrain_idx}; feature_split_num: {feature_split_num}; sub_features_num: {sub_features_num}')\n",
    "    start_idx = 0\n",
    "    for sub_features_idx in range(feature_split_num):\n",
    "        if sub_features_idx == feature_split_num - 1:\n",
    "            end_idx = len(FEATURES)\n",
    "        else:\n",
    "            end_idx = start_idx+sub_features_num\n",
    "        sub_features = FEATURES[start_idx: end_idx]\n",
    "        # Train\n",
    "        acc, oof, importances = train_xgb_params(params_13m, \n",
    "                                                 train,\n",
    "                                                 features=sub_features+['random'],\n",
    "                                                 val_folds=val_folds,\n",
    "                                                 early_stopping_rounds=100, \n",
    "                                                 num_boost_round=9999,  \n",
    "                                                 verbose_eval=False, \n",
    "                                                 data_seed=SEED, \n",
    "                                                 model_seed=SEED, \n",
    "                                                 save_dir=permu_dir,\n",
    "                                                 ver=f'13M_{retrain_idx}_{sub_features_idx}')\n",
    "        importances.to_csv(os.path.join(permu_dir, 'original_importances.csv'))\n",
    "        # Permutation Importance\n",
    "        permutation_importance_list = []\n",
    "        for fold in val_folds:\n",
    "            print(f'........ sub_features_idx: {sub_features_idx}; start_idx: {start_idx}, end_idx: {end_idx}, fold: {fold};')\n",
    "            model_file = [c for c in os.listdir(permu_dir) if f'13M_{retrain_idx}_{sub_features_idx}_fold{fold}' in c]\n",
    "            if len(model_file) > 1:\n",
    "                raise ValueError(f'There are more than one model file: {model_file}')\n",
    "            model_file = model_file[0]\n",
    "            model = xgb.Booster()\n",
    "            model_path = os.path.join(permu_dir, model_file)\n",
    "            # PERMUTATION IMPORTANCE\n",
    "            importances_fold = importances[f'importance_{fold}']\n",
    "            shuffle_features = importances_fold[importances_fold.notnull()].index.to_list()\n",
    "            permutation_importance = get_permutation_importance(model_path,\n",
    "                                                                train.loc[train['fold'] == fold, :],\n",
    "                                                                model_features=sub_features+['random'], \n",
    "                                                                shuffle_features=shuffle_features,\n",
    "                                                                shuffle_times=shuffle_times) \n",
    "            permutation_importance['retrain_idx'] = retrain_idx                                                   \n",
    "            permutation_importance['sub_features_idx'] = sub_features_idx                                                    \n",
    "            permutation_importance['fold'] = fold\n",
    "            permutation_importance.to_csv(os.path.join(permu_dir, f'permutation_importance_{retrain_idx}_{sub_features_idx}_fold{fold}.csv'), index=False)        \n",
    "            permutation_importance_list.append(permutation_importance)\n",
    "        start_idx = end_idx\n",
    "    permutation_importance_agg = agg_permu_files(permu_dir, count_threshold=len(val_folds)*shuffle_times, retrain_idx=retrain_idx)\n",
    "    drop_features = permutation_importance_agg[permutation_importance_agg['z'] < permutation_importance_agg['random_z']].index.to_list()\n",
    "    permutation_importance_agg['is_drop'] = permutation_importance_agg.index.isin(drop_features)\n",
    "    permutation_importance_agg.to_csv(os.path.join(permu_dir, f'permutation_importance_agg_{retrain_idx}.csv'))\n",
    "    FEATURES = [c for c in FEATURES if c not in drop_features]\n",
    "    retrain_idx += 1\n",
    "    break\n",
    "    # if len(drop_features) == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70973d-5ee1-407c-a336-f1bb26b32342",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "746df887-26d9-4de2-a451-0cd9d97d2f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features 300 auc 0.9597397304150984\n"
     ]
    }
   ],
   "source": [
    "val_folds = [0,1,2,3,4,5,6,7,8,9]\n",
    "acc, oof, importances = train_xgb_params(params_13m, \n",
    "                                     train,\n",
    "                                     features=FEATURES,\n",
    "                                     val_folds=val_folds,\n",
    "                                     early_stopping_rounds=100, \n",
    "                                     num_boost_round=9999,  \n",
    "                                     verbose_eval=False, \n",
    "                                     data_seed=SEED, \n",
    "                                     model_seed=SEED, \n",
    "                                     save_dir=False)\n",
    "print('num_features', len(FEATURES), 'auc', acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd4152-25b5-408f-8632-859b999f48d9",
   "metadata": {},
   "source": [
    "# 筛选变量实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d277bf8f-33e3-40a7-b849-b384b71307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folds = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bbcb1-fe80-4539-8628-aecd0b0dce40",
   "metadata": {},
   "source": [
    "## 默认的Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cac88cc3-e964-4847-8da0-1a155cec3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_importances = pd.read_csv(os.path.join(permu_dir, 'original_importances.csv'))\n",
    "original_importances = original_importances.set_index('feature')\n",
    "original_importances['mean'] = original_importances.mean(axis=1)\n",
    "original_importances['std'] = original_importances.std(axis=1)\n",
    "original_importances.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ae9b63e-45fc-4aeb-ba18-bb2faaa0692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features 25 auc 0.9519786413209202\n",
      "num_features 50 auc 0.9551949305682249\n",
      "num_features 75 auc 0.957583263622448\n",
      "num_features 100 auc 0.9581601072672994\n",
      "num_features 125 auc 0.958722652426215\n",
      "num_features 150 auc 0.959060598689679\n",
      "num_features 175 auc 0.9593140320399974\n",
      "num_features 200 auc 0.9595746364492562\n",
      "num_features 225 auc 0.9596106789864496\n",
      "num_features 250 auc 0.959798236138387\n"
     ]
    }
   ],
   "source": [
    "features_mean = original_importances.sort_values('mean', ascending=False)['feature'].to_list()\n",
    "num_features = 25\n",
    "while True:\n",
    "    acc, oof, importances = train_xgb_params(params_13m, \n",
    "                                         train,\n",
    "                                         features=features_mean[:num_features],\n",
    "                                         val_folds=val_folds,\n",
    "                                         early_stopping_rounds=100, \n",
    "                                         num_boost_round=9999,  \n",
    "                                         verbose_eval=False, \n",
    "                                         data_seed=SEED, \n",
    "                                         model_seed=SEED, \n",
    "                                         save_dir=False)\n",
    "    print('num_features', num_features, 'auc', acc) \n",
    "    num_features += 25\n",
    "    if num_features > permutation_importance_agg.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb9abc-f468-441b-8581-07b9b93117c6",
   "metadata": {},
   "source": [
    "## 平均下降比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b3e1beb-0387-431f-b831-0b4fcc92912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features 25 auc 0.955488431492996\n",
      "num_features 50 auc 0.9582052598376949\n",
      "num_features 75 auc 0.9589016974455882\n",
      "num_features 100 auc 0.9593989000375628\n",
      "num_features 125 auc 0.9595675126936822\n",
      "num_features 150 auc 0.9597684801181003\n",
      "num_features 175 auc 0.9598145119359532\n",
      "num_features 200 auc 0.9597407810213192\n",
      "num_features 225 auc 0.9598664237091002\n",
      "num_features 250 auc 0.959691966613858\n"
     ]
    }
   ],
   "source": [
    "permutation_importance_agg = pd.read_csv(os.path.join(permu_dir, 'permutation_importance_agg_1.csv'))\n",
    "features_mean = permutation_importance_agg.sort_values('mean', ascending=False)['feature'].to_list()\n",
    "num_features = 25\n",
    "while True:\n",
    "    acc, oof, importances = train_xgb_params(params_13m, \n",
    "                                         train,\n",
    "                                         features=features_mean[:num_features],\n",
    "                                         val_folds=val_folds,\n",
    "                                         early_stopping_rounds=100, \n",
    "                                         num_boost_round=9999,  \n",
    "                                         verbose_eval=False, \n",
    "                                         data_seed=SEED, \n",
    "                                         model_seed=SEED, \n",
    "                                         save_dir=False)\n",
    "    print('num_features', num_features, 'auc', acc) \n",
    "    num_features += 25\n",
    "    if num_features > permutation_importance_agg.shape[0]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f0727-305f-4d00-b462-31c0816c2c74",
   "metadata": {},
   "source": [
    "## 标准化后的平均下降比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba5f49e-d30b-45a7-8169-97cd959045ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features 25 auc 0.9556982881629269\n",
      "num_features 50 auc 0.9584120117990971\n",
      "num_features 75 auc 0.9589902318791576\n",
      "num_features 100 auc 0.9595072118772046\n",
      "num_features 125 auc 0.9597150474489736\n",
      "num_features 150 auc 0.9597403596967278\n",
      "num_features 175 auc 0.9599117083347268\n",
      "num_features 200 auc 0.9599500114626974\n",
      "num_features 225 auc 0.9598069184316719\n",
      "num_features 250 auc 0.959729845054988\n"
     ]
    }
   ],
   "source": [
    "permutation_importance_agg = pd.read_csv(os.path.join(permu_dir, 'permutation_importance_agg_1.csv'))\n",
    "features_z = permutation_importance_agg.sort_values('z', ascending=False)['feature'].to_list()\n",
    "num_features = 25\n",
    "while True:\n",
    "    acc, oof, importances = train_xgb_params(params_13m, \n",
    "                                         train,\n",
    "                                         features=features_z[:num_features],\n",
    "                                         val_folds=val_folds,\n",
    "                                         early_stopping_rounds=100, \n",
    "                                         num_boost_round=9999,  \n",
    "                                         verbose_eval=False, \n",
    "                                         data_seed=SEED, \n",
    "                                         model_seed=SEED, \n",
    "                                         save_dir=False)\n",
    "    print('num_features', num_features, 'auc', acc) \n",
    "    num_features += 25\n",
    "    if num_features > permutation_importance_agg.shape[0]:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
